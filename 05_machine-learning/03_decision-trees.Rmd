
```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(dplyr)
library(ggplot2)
theme_set(theme_bw(base_size = 16))
library(readr)
```

# Decision Trees

We have described two types of machine learning algorithms. Linear approaches (generalized linear models (Logistic Regression), discriminant analysis (QDA, LDA)) and a smoothing approach (k-nearest neighbors). The linear approaches were limiting in that the partition of the prediction space had to be linear (in the case of QDA, quadratic). A limitation of the smoothing approach is that with a large number of predictors, we run into the problem of _the curse of dimensionality_.

## The Curse of Dimensionality

The curse of dimensionality refers to various phenomena that arise when analyzing and organizing data in high-dimensional spaces that do not occur in low-dimensional settings such as the three-dimensional physical space of everyday experience. 

The common theme of these problems is that when the dimensionality increases, the volume of the space increases so fast that the available data become sparse. In order to obtain a reliable result, the amount of data needed often grows exponentially with the dimensionality. Also, organizing and searching data often relies on detecting areas where objects form groups with similar properties; in high dimensional data, however, all objects appear to be sparse and dissimilar in many ways, which prevents common data organization strategies from being efficient.

Here we look at a set of elegant and versatile methods that adapt to higher dimensions and also allow these regions to take more complex shapes, but still produce models that are interpretable. These are very popular, well-known and studied methods. We will concentrate on Regression and Decision Trees and their extension to Random Forests.

### Regression Trees

Consider the olives dataset below. We show two measured predictors, `linoleic` (percent linoleic acid of sample) and `eicosenoic` (percent eicosenoic acid of sample). Suppose we wanted to predict the olive's region using these two predictors.

```{r, message=FALSE, warning=FALSE}
olives <- read.csv("https://raw.githubusercontent.com/datasciencelabs/data/master/olive.csv", as.is = TRUE) %>% tbl_df
names(olives)[1] <- "province"
region_names <- c("Southern Italy", "Sardinia", "Northern Italy")
olives <- olives %>% mutate(Region = factor(region_names[Region]))

p <- olives %>% ggplot(aes(eicosenoic, linoleic, fill = Region)) +
     geom_point(pch = 21)
p
```

Note that we can describe a classification algorithm that would work pretty much perfectly:

```{r}
p <- p + geom_vline(xintercept = 6.5) + 
         geom_segment(x = -2, y = 1053.5, xend = 6.5, yend = 1053.5)
p
```

The prediction algorithm inferred from the figure above is what we call a decision tree. If eicosnoic is larger than 6.5, predict Southern Italy. If not, then if linoleic is larger than 1,054, predict Sardinia and Northern Italy otherwise. We can draw this decision tree like this:

```{r, echo=FALSE}
library(rpart)
library(rpart.plot)

olives <- olives %>% dplyr::select(., Region, linoleic, eicosenoic)

fit <- rpart(as.factor(Region) ~ ., data = olives)

rpart.plot(fit)
```

Decision trees like this are often used in practice. For example, to decide if a person is at risk of having a heart attack, doctors use the following:

![](http://nargund.com/gsu/mgs8040/resource/dss/Simple%20Heuristics%20That%20Make%20Us%20Smart_files/todd.fig1.gif)


The general idea of the methods we are describing is to define an algorithm that uses data to create these tress. Regression and decision trees operate by predicting an outcome variable $Y$ by **partitioning** feature (predictor) space.

## Regression Trees

Let's start with the case of a **continuous** outcome. The general idea here is to build a decision tree and at the end of each _node_ we will have a different prediction $\hat{Y}$ for the outcome $Y$.

The **regression** tree model then:

1. Partitions space into $J$ non-overlapping regions, $R_1, R_2, \ldots, R_J$.
2. For every observation that falls within region $R_j$, predict the response as the mean of responses for training observations in $R_j$.

The important observation is that **Regression Trees create partitions recursively**.

For example, consider finding a good predictor $j$ to partition space along its axis. A recursive algorithm would look like this:

Find predictor $j$ and value $s$ that minimize RSS:

$$
\sum_{i:\, x_i \in R_1(j,s))} (y_i - \hat{y}_{R_1})^2 +
\sum_{i:\, x_i \in R_2(j,s))} (y_i - \hat{y}_{R_2})^2
$$

Where $R_1$ and $R_2$ are regions resulting from splitting observations on predictor $j$ and value $s$:

$$
R_1(j,s) = \{X|X_j < s\} \text{ and } R_2(j,s) \{X|X_j \geq s\}
$$

This is then applied recursively to regions $R_1$ and $R_2$. Within each region a prediction is made using $\hat{y}_{R_j}$ which is the mean of the response $Y$ of observations in $R_j$.

Let's take a look at what this algorithm does on the `Boston Housing` data set. This dataset contains information collected by the U.S Census Service concerning housing in the area of Boston, MA in the 1970s. It was obtained from the [StatLib archive](http://lib.stat.cmu.edu/datasets/boston). It contains information including `medv` (median value of owner-occupied homes in $1000's), `lstat` (% of individuals with lower socioeconomic status), `rm` (average number of rooms per dwelling), and `dis` (weighted distances to five Boston employment centres), among others with a total of 14 variables. 

The dataset is small in size with only 506 cases, but we'll use it for educational purposes.

We can use the `tree` package and use the `tree` function to fit the decision tree and plot the tree.

```{r, message=FALSE, warning=FALSE}
library(tree)
library(MASS)

set.seed(1)

# Randomly sample half of the data for training
train = sample(1:nrow(Boston), nrow(Boston)/2)

# Fit a regression tree using all of the available predictors
fit = tree(medv ~ ., Boston, subset = train)  

# Print a summary of the tree
summary(fit)

# Use tree for prediction
preds <- predict(fit, newdata = Boston[-train,])
test = Boston[-train, "medv"]
```

```{r}
plot(fit, type = "uniform")
text(fit, cex = 1)
```

The tree suggests that houses with a higher number of rooms correspond to more expensive houses and predicts a median house price of $45,380 for a house with more than 8 rooms. 

The idea behind the regression tree is that outcome $Y$ is estimated (or predicted) to be it's mean _within each of the data partitions_. Think of it as the conditional mean of $Y$ where conditioning is given by this region partitioning. 

We can also use the predictions made to calculate MSE (mean square error) to compare models. 
```{r}
plot(preds, test)
abline(0,1)
mean((preds-test)^2)
```

The test set MSE associated with this tree is 35.29. The square root of the MSE is 5.94, indicating this model leads to test predictions that are within approximately $5,940 of the true median home value for the suburb.


The `rpart` package is an alternative method for fitting trees in R. It is much more feature rich, including fitting multiple cost complexities and performing cross-validation by default. It also has the ability to produce much nicer trees. Based on its default settings, it will often result in smaller trees than using the tree package.

```{r, message=FALSE, warning=FALSE}
library(rpart)
set.seed(1)

 # Fit a regression tree using all of the available predictors
fit_rpart = rpart(medv ~ ., Boston, subset = train)  

# Print a summary of the tree
summary(fit_rpart)

# Use tree for prediction
preds <- predict(fit_rpart, newdata = Boston[-train,])
test = Boston[-train, "medv"]
```

Let's plot the resulting tree:

```{r}
rpart.plot(fit_rpart, digits = 4)
```

We get the same predictions as we did using the `tree` function. We can also calculate the MSE and check that it is also the same:

```{r}
mean((preds-test)^2)
```



### Specifics of the regression tree algorithm

The recursive partitioning algorithm described above leads to a set of natural questions:

1. _When do we stop partitioning?_ 

We stop when adding a partition does not reduce MSE, or, when a partition has too few training observations. Even then, trees built with this stopping criterion tend to _overfit_ to the training data. To avoid this, a post-processing step called **pruning** is used to make the tree smaller.

2. Why would a smaller tree tend to generalize better?

The `cv.tree` function is used to determine a reasonable tree depth for the given dataset. For this dataset it seems that a depth of 7 works well since it reaches the minimum error or "deviance" with that number:

```{r}
set.seed(1)
cv_boston = cv.tree(fit)
plot(cv_boston$size, cv_boston$dev, type = 'b')
```

However, if we decide to prune a tree we can do so using the `prune.tree()` function:

```{r}
prune_boston = prune.tree(fit, best = 5)
plot(prune_boston, type = "uniform")
text(prune_boston)
```

Let's calculate the test set MSE for the pruned tree:

```{r}
preds_prune <- predict(prune_boston, newdata = Boston[-train,])
test = Boston[-train, "medv"]
mean((preds_prune-test)^2)
```

The MSE of the pruned tree is greater than the original, so we should not prune this decision tree.

We can also prune a tree using the `prune` function from the `rpart` package. Just as we did with the `tree` function above, we can first plot the error for different tree sizes. The error is the smallest with 7 leaves or terminal nodes. But, we can prune the tree to 5 terminal nodes using a _complexity parameter_ (`cp`) of 0.037. 

```{r}
set.seed(1)
fit_rpart = rpart(medv ~ ., Boston, subset = train)  

# Use tree for prediction
preds <- predict(fit_rpart, newdata = Boston[-train,])
test = Boston[-train, "medv"]

plotcp(fit_rpart)
min_cp = fit_rpart$cptable[which.min(fit_rpart$cptable[,"xerror"]),"CP"]
min_cp
```

```{r}
p <- prune(fit_rpart, cp = 0.037)
rpart.plot(p, digits = 4)
```

### Classification (Decision) Trees

Classification, or decision trees, are used in classification problems where the outcome is categorical. The same partitioning principle is used, but now each region predicts the **majority** class for training observations within that region. The recursive partitioning algorithm we saw previously requires a score function to choose predictors (and values) to partition with. In classification we could use a naive approach of looking for partitions that minimize training error. However, better performing approaches use more sophisticated metrics. Here are two of the most popular (denoted for leaf $m$):
 
  - **Gini Index**: $\sum_{k=1}^K \hat{p}_{mk}(1-\hat{p}_{mk})$, or
  
  - **Entropy**: $-\sum_{k=1}^K \hat{p}_{mk}\log(\hat{p}_{mk})$
  
where $\hat{p}_{mk}$ is the proportion of training observations in partition $m$ labeled as class $k$. Both of these seek to partition observations into subsets that have the same labels.

Let us look at how a classification tree performs on the digits example we examined before:

```{r, echo=FALSE, message=FALSE, warning=FALSE}
detach("package:MASS", unload=TRUE)
url <- "https://raw.githubusercontent.com/datasciencelabs/data/master/hand-written-digits-train.csv"
if(!exists("digits")) digits <- read_csv(url)
digits <- digits %>% filter(label %in% c(2,7))
digits <- mutate(digits, label =  as.character(label)) %>% 
          mutate(label = ifelse(label=="2",0,1 ))
row_column <- expand.grid(row=1:28, col=1:28)
ind1 <- which(row_column$col <= 14 & row_column$row <=14)
ind2 <- which(row_column$col > 14 & row_column$row > 14)
ind <- c(ind1, ind2)
X <- as.matrix(digits[,-1])
X <- X>200
X1 <- rowSums(X[,ind1])/rowSums(X)
X2 <- rowSums(X[,ind2])/rowSums(X)
digits <- mutate(digits, X_1 = X1, X_2 = X2) %>% dplyr::select(., label, X_1, X_2) %>%
  mutate(label = as.factor(label))
y <- digits$label
x <- cbind(X1, X2)
library(caret)
fit <- knn3(x, y, 51)
GS <- 150
X1s <- seq(min(X1),max(X1),len=GS)
X2s <- seq(min(X2),max(X2),len=GS)
true_f <- expand.grid(X_1=X1s, X_2=X2s)
yhat <- predict(fit, newdata = true_f, type="prob")[,2]
true_f <- mutate(true_f, yhat=yhat)
f <- loess(yhat~X_1*X_2, data=true_f, 
           degree=1, span=1/5)$fitted
true_f <- true_f %>% mutate(f=f) 
rm(X,X1,X2,fit,GS,X1s,X2s,yhat,f)

true_f_plot <- true_f %>%
  ggplot(aes(X_1, X_2, fill=f))  +
  scale_fill_gradientn(colors=c("#F8766D","white","#00BFC4")) + geom_raster()  + guides(fill=FALSE) +  
  stat_contour(aes(x=X_1,y=X_2,z=f), data=true_f, breaks=c(0.5),color="black",lwd=1.5)
true_f_plot
```


```{r, fig.width=14, fig.height=6, echo = FALSE}
set.seed(1)

inTrain <- createDataPartition(y = digits$label, p = 0.5, times = 1, list = FALSE)
digits_train <- slice(digits, inTrain)
digits_test  <- slice(digits, -inTrain)

fit <- tree(label ~ X_1 + X_2, data = digits_train)
plot(fit, type = "uniform")
text(fit, cex = 2, digits = 2)
```

```{r, fig.width=14, fig.height=6, echo = FALSE}
set.seed(1)

fit_rpart <- rpart(label ~ X_1 + X_2, data = digits_train)
rpart.plot(fit_rpart, digits = 4)
```


We can see the prediction here:

```{r, message=FALSE, warning=FALSE, echo=FALSE}
f_hat_cart <- predict(fit, newdata = true_f)[,2]
p <-true_f %>% mutate(f = f_hat_cart) %>%
               ggplot(aes(X_1, X_2, fill = f)) +
               scale_fill_gradientn(colors = c("#F8766D","white","#00BFC4")) + 
               geom_raster() +  
               stat_contour(aes(x = X_1, y = X_2, z = f), 
               data = mutate(true_f, f = f_hat_cart),
               breaks = c(0.5), color = "black", lwd = 1.5) +
               guides(fill = FALSE)

library(gridExtra)
grid.arrange(true_f_plot, p, nrow=1)
```

We can again prune the tree if we wish, but in this case the pruned tree does not differ much from the original.

```{r}
pruned_fit <- prune.tree(fit)
plot(pruned_fit)
```

Here is what a pruned tree looks like:

```{r, message=FALSE, warning=FALSE, echo=FALSE}
pruned_fit  <- prune.tree(fit, k = 160)
f_hat_cart2 <- predict(pruned_fit, newdata = true_f)[,2]
p <-true_f %>% mutate(f = f_hat_cart2) %>%
               ggplot(aes(X_1, X_2, fill = f)) +
               scale_fill_gradientn(colors = c("#F8766D","white","#00BFC4")) + 
               geom_raster() +  
               stat_contour(aes(x = X_1, y = X_2, z = f), 
               data = mutate(true_f, f = f_hat_cart2),
               breaks = c(0.5),color = "black", lwd = 1.5)
p
```



Classification trees have certain advantages that make them very useful. They are highly interpretable, even more so than linear models, are easy to visualize (if small enough), and they (maybe) model human decision processes and don't require that dummy predictors for categorical variables are used.

On the other hand, the greedy approach via recursive partitioning is a bit harder to train than linear regression. It may not always be the best performing method since it is not very flexible and is highly unstable to changes in training data. Below we will learn about the bootstrap to help with this.


## Random Forests

Random Forests are a **very popular** approach that address the shortcomings of decision trees via re-sampling of the training data. Their goal is to improve prediction performance and reduce instability by _averaging_ multiple decision trees (a forest constructed with randomness). It has two features that help accomplish this.

The first trick is *Bagging* (bootstrap aggregation)
General scheme:

  1. Build many decision trees $T_1, T_2, \ldots, T_B$ from training set
  2. Given a new observation, let each $T_j$ predict $\hat{y}_j$
  3. For regression: predict average $\frac{1}{B} \sum_{j=1}^B \hat{y}_j$,
     for classification: predict with majority vote (most frequent class)
     
But how do we get many decision trees from a single training set?

For this we use the _bootstrap_. To create $T_j, \, j=1,\ldots,B$ from a training set of size $N$:

a) Create a bootstrap training set by sampling $N$ observations from training set **with replacement**
b) Build a decision tree from the bootstrap training set

Let's look at this using the Boston housing dataset. We fit a Random Forest by using the `randomForest()` function. Here, `mtry = 13` indicates all 13 predictors should be considered for each split of the tree - in other words, bagging should be done. 

```{r, message=FALSE, warning=FALSE}
library(randomForest)
library(MASS)
fit_bag <- randomForest(medv ~ ., data = Boston[train,], mtry = 13)
fit_bag
```

How well does it perform on the test set?
```{r}
preds_bag = predict(fit_bag, newdata = Boston[-train,])
plot(preds_bag, test)
abline(0,1)
mean((preds_bag - test)^2)
```

Much, much better than the one decision tree we used above. With Random Forest we get an MSE of 23.4 which (after square rooting) translates to our predictions being within $4,847 of the true median home value for the suburb. 
This is much lower than the $5,940 we got when using the one decision tree.

We can grow a random forest in the same way, except now we use a smaller value for `mtry`. The default for regression trees is $p/3$ ($p$ is the number of predictors) and the default for classification trees is $\sqrt p$. Let's try `mtry = 6`.

```{r}
fit_rf <- randomForest(medv ~ ., data = Boston[train,], mtry = 6, importance = TRUE)
preds_rf = predict(fit_rf, newdata = Boston[-train,])
mean((preds_rf - test)^2)
```

Looks like the MSE for this random forest is better with fewer predictors and the one using all of the predictors performs worse.

The second Random Forest feature is to use a random selection of features to split when deciding partitions. Specifically, when building each tree $T_j$, at each recursive partition only consider a _randomly selected subset of predictors_ to check for the best split. This reduces correlation between trees in a forest, improving prediction accuracy.

Here is the random forest fit for the digits data:

```{r, warning=FALSE, message=FALSE}
detach("package:MASS", unload=TRUE)
library(randomForest)
fit <- randomForest(label ~ X_1 + X_2, data = digits_train)
```

```{r, message=FALSE, warning=FALSE, echo=FALSE}
f_hat_rf <- predict(fit, newdata = true_f, type = "prob")[,2]
p <-true_f %>% mutate(f = f_hat_rf) %>%
               ggplot(aes(X_1, X_2, fill = f)) +
               scale_fill_gradientn(colors = c("#F8766D","white","#00BFC4")) + 
               geom_raster() + 
               stat_contour(aes(x = X_1, y = X_2, z = f), 
               data = mutate(true_f, f = f_hat_rf),
               breaks = c(0.5), color = "black", lwd = 1.5) +
               guides(fill = FALSE)

library(gridExtra)
grid.arrange(true_f_plot, p, nrow = 1)
```

We can control the "smoothness" of the random forest estimate in several ways. One way is to limit the size of each node. We can require the number of points per node to be larger:

```{r}
fit <- randomForest(as.factor(label) ~ X_1  +X_2,
                    nodesize = 250,
                    data = digits_train)
```

```{r, message=FALSE, warning=FALSE, echo=FALSE}
f_hat_rf <- predict(fit, newdata = true_f, type = "prob")[,2]

p <-true_f %>% mutate(f = f_hat_rf) %>%
               ggplot(aes(X_1, X_2, fill = f))  +
               scale_fill_gradientn(colors = c("#F8766D","white","#00BFC4")) + 
               geom_raster() +  
               stat_contour(aes(x = X_1, y = X_2, z = f), 
               data = mutate(true_f, f = f_hat_rf),
               breaks = c(0.5), color = "black", lwd = 1.5) +
               guides(fill = FALSE)

p
```


We can compare the results:

```{r}
library(caret)
get_accuracy <- function(fit){
  pred <- predict(fit, newdata = digits_test, type = "class")
  confusionMatrix(table(pred = pred, true = digits_test$label))$overall[1]
}
fit <- tree(label ~ X_1 + X_2, data = digits_train)
get_accuracy(fit)

fit <- randomForest(label ~ X_1 + X_2, data = digits_train)
get_accuracy(fit)

fit <- randomForest(label ~ X_1 + X_2,
                    nodesize = 250,
                    data = digits_train)
get_accuracy(fit)
```


A disadvantage of random forests is that we lose interpretability. However, we can use the fact that a bootstrap sample was used to construct trees to measure _variable importance_ from the random forest. The importance of a variable translates to how predictive that variable is. 

Let's see this using all the digits data:

```{r, message=FALSE, warning=FALSE}
url <- "https://raw.githubusercontent.com/datasciencelabs/data/master/hand-written-digits-train.csv"
digits <- read_csv(url)

digits <- mutate(digits, label = as.factor(label))

inTrain   <- createDataPartition(y = digits$label, p = 0.9, times = 1, list = FALSE)
train_set <- slice(digits, inTrain)
test_set  <- slice(digits, -inTrain)

fit <- randomForest(label~., ntree = 100, data = train_set)
```

How well does it do?

```{r}
pred <- predict(fit, newdata = test_set, type = "class")
confusionMatrix(table(pred = pred, true = test_set$label))
```

Here is a table of _variable importance_ for the random forest we just constructed.


```{r, echo=TRUE, results="asis"}
library(knitr)
variable_importance <- importance(fit) 
tmp <- tibble(feature = rownames(variable_importance),
                  Gini = variable_importance[,1]) %>%
                  arrange(desc(Gini))
kable(tmp[1:10,])
```

We can see where the "important" features are:

```{r}
expand.grid(Row = 1:28, Column = 1:28) %>%
            mutate(value = variable_importance[,1]) %>%
            ggplot(aes(Row, Column, fill = value)) +
            geom_raster() +
            scale_y_reverse() 
```


And a barplot of the same data showing only the most predictive features:

```{r}
tmp %>% filter(Gini > 200) %>%
        ggplot(aes(x=reorder(feature, Gini), y=Gini)) +
        geom_bar(stat='identity') +
        coord_flip() + xlab("Feature") +
        theme(axis.text=element_text(size=8))
```

### Tree-based methods summary

Tree-based methods are very interpretable _prediction_ models for which some inferential tasks are possible (e.g., variable importance in random forests), but are much more limited than the linear models we saw previously. These methods are very commonly used across many application domains and Random Forests often perform at state-of-the-art for many tasks.
