---
title: 'Lab 6: Machine Learning II'
output:
  html_document
---

## kNN, LDA, and QDA for Binary Classification
This lab will focus on kNN, QDA, LDA, and ROC curves.

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(mvtnorm)
library(caret)
library(MASS)
library(pROC)
```

In this section, we simulate a dataset that has 200 observations from Class 0 and 200 observations from Class 1. The class labels are stored in the variable Y. There are also two predictor variables, X1 and X2. When an observation is from Class 0, X1 and X2 are drawn from a bivariate normal distribution with mean $\mu_0 = \left( \begin{array}{c} 3 \\ 2 \end{array} \right)$ and covariance matrix $\Sigma = \left( \begin{array}{cc} 1 & 0.5 \\ 0.5 & 1 \end{array} \right)$. When an observation is from Class 1, X1 and X2 are drawn from a bivariate normal distribution with mean $\mu_1 = \left( \begin{array}{c} 4 \\ 4 \end{array} \right)$ and the same covariance matrix $\Sigma = \left( \begin{array}{cc} 1 & 0.5 \\ 0.5 & 1 \end{array} \right)$. 

In slightly less technical terms, this means that when an observation is from Class 0, X1 is normally distributed with mean 3 and standard deviation 1; X2 is normally distributed with mean 2 and standard deviation 1; and the two predictors have a moderately positive correlation. When an observation is from Class 1, X1 and X2 are still normally distributed with the same correlation structure as in Class 0, but their means are 4 instead. 

```{r}
# Means for two classes
mu0 = c(3, 2)
mu1 = c(4, 4)
# Shared covariance matrix.
Sigma = matrix(c(1, 0.5, 0.5, 1), nrow = 2)

# Simulate 200 observations from each class and create vector of labels
set.seed(1)
sim_dat = data.frame(Y = factor(rep(c(0, 1), each = 200)), 
                     rbind(mvrnorm(200, mu0, Sigma), 
                           mvrnorm(200, mu1, Sigma)))

# Take a peek at the simulated data
head(sim_dat)
```

### Question 1
Use the `createDataPartition` function from the `caret` package to split the data into training and test sets (50% training and 50% test). Be sure to set a random seed so that your code is reproducible (the solutions use `set.seed(4)`). Visualize the training observations by making a scatterplot of X2 against X1 and coloring the points by Y. Comment on what you see. 

**Solution:** 

We split the data so that half of it is designated for training and the other half is set aside for testing. 

```{r}
set.seed(4)
sim_index_train = createDataPartition(y = sim_dat$Y, 
                                      times = 1, p = 0.5, list = FALSE)
sim_train_set = slice(sim_dat, sim_index_train)
sim_test_set = slice(sim_dat, -sim_index_train)
```

In coloring the points by Y, we can see that the two classes form somewhat distinct groups in the scatterplot. However, there is still some overlap between the two classes. 

```{r}
ggplot() +
  geom_point(aes(x = X1, y = X2, color = Y), data = sim_train_set) + 
  scale_shape_manual(values = c(16, 3))
```


### Question 2
Use the k-Nearest Neighbors (kNN) algorithm with k=1 neighbors to model Y based on X1 and X2 in the training data. You can use the `knn3` function from the `caret` package. Save the output from `knn3` as `fit_knn1`. Then, run kNN with k=15 neighbors and save your model as `fit_knn15`. For each model, obtain the predicted probabilities of belonging in Class 1 for the observations in the test set. 

**Solution:** 

First, we fit kNN models with k=1 and k=15 neighbors, specified using the `k` parameter in the `knn3` function. 

```{r}
fit_knn1 = knn3(Y ~ X1 + X2, data = sim_train_set, k = 1)
fit_knn15 = knn3(Y ~ X1 + X2, data = sim_train_set, k = 15)
```

Then, we can use the fitted models to predict on the test set. By default, the `predict` function for `knn3` models returns predicted probabilities, but you can also explicitly specify `type = "prob"`. The `predict` function returns a matrix with two columns (one set of probabilities for each class), and since we only want the probabilities for belonging in Class 1, we'll just keep the second column. 

```{r}
probs_knn1 = predict(fit_knn1, newdata = sim_test_set)[,2]
probs_knn15 = predict(fit_knn15, newdata = sim_test_set)[,2]
```


### Question 3
Use the `lda` and `qda` functions from the `MASS` package to fit linear discriminant analysis (LDA) and quadratic discriminant analysis (QDA) models to the training data. Save your models as `fit_lda` and `fit_qda`, respectively. Both X1 and X2 should be included as predictors. For each model, obtain the predicted probabilities of belonging in Class 1 for the observations in the test set. 

**Solution:** 

We can fit LDA and QDA models using the `lda` and `qda` functions from the `MASS` package. 

```{r}
fit_lda = lda(Y ~ X1 + X2, data = sim_train_set)
fit_qda = qda(Y ~ X1 + X2, data = sim_train_set)
```

The `predict` function for `lda` and `qda` returns a list with several components, but we can extract the predicted probabilities from the `posterior` slot. As before, we subset the resulting two-column matrix to only keep the probabilities for belonging in Class 1. 

```{r}
probs_lda = predict(fit_lda, newdata = sim_test_set)$posterior[,2]
probs_qda = predict(fit_qda, newdata = sim_test_set)$posterior[,2]
```


### Question 4
Using the `pROC` package and the predicted probabilities from the four models that you fit in Q2-3, plot and interpret the receiver operating characteristic (ROC) curves based on the test set. Also, report the area under the curve (AUC) for each of the four models. 

**Solution:** 

Plotting ROC curves using the `pROC` package first requires creating "roc" objects by running the `roc` function. We can do this by passing in the true class labels from the test set and the predicted probabilities calculated from each of the four models. 

```{r, message=FALSE, warning=FALSE}
roc_knn1 = roc(sim_test_set$Y, probs_knn1)
roc_knn15 = roc(sim_test_set$Y, probs_knn15)
roc_lda = roc(sim_test_set$Y, probs_lda)
roc_qda = roc(sim_test_set$Y, probs_qda)
```

The curves for LDA, QDA, and kNN with k=15 all hug the upper-left corner closely, so these models are doing a good job of discriminating between the two classes in the test set. kNN with k=1 appears to be doing noticeably worse. 

```{r}
ggroc(list("KNN (k = 1)" = roc_knn1, 
           "KNN (K = 15)" = roc_knn15,
           "LDA" = roc_lda,
           "QDA" = roc_qda)) +
  theme(legend.title = element_blank()) +
  geom_segment(aes(x = 1, xend = 0, y = 0, yend = 1), color = "black", linetype = "dashed") +
  ylab("Sensitivity") +
  xlab("Specificity") 
```

We can calculate the AUC for each of the four models using the `auc` function. The inputs are the same as they were for `roc`. 

```{r, message=FALSE, warning=FALSE}
auc(sim_test_set$Y, probs_knn1)
auc(sim_test_set$Y, probs_knn15)
auc(sim_test_set$Y, probs_lda)
auc(sim_test_set$Y, probs_qda)
```

Alternatively, because we already ran `roc` for each of the four models, we can extract the AUC information directly from our saved "roc" objects. 

```{r, message=FALSE, warning=FALSE}
roc_knn1$auc
roc_knn15$auc
roc_lda$auc
roc_qda$auc
```

LDA and QDA have nearly identical AUCs of about 0.921, and are followed closely behind by kNN with k=15 (AUC of 0.911). A model that is classifying perfectly has an AUC of 1 and a model that is randomly guessing has an AUC of around 0.5, so these three models are doing quite well. kNN with k=1 is having a harder time discriminating between classes, as evidenced by both the shape of the ROC curve and its lower AUC of 0.770. 



### Question 5
The code below plots the test set data, as well as decision boundaries calculated from the `fit_knn`, `fit_knn15`, `fit_lda`, and `fit_qda` models that you built in Q2-3. To run, remove the `eval = FALSE` option from the code chunk header. Discuss how the assumptions of the models and the data-generating process are reflected in the shapes of the decision boundaries. 

```{r}
# Create a grid of X1 and X2 values that span the space of the test set
grid <- expand.grid(seq(min(sim_test_set$X1), 
                        max(sim_test_set$X1), length=200), 
                    seq(min(sim_test_set$X2), 
                        max(sim_test_set$X2), length=200))
colnames(grid) <- c("X1", "X2")

# Predict class labels on all values in the grid for each model
boundary_df <- 
  data.frame(
    grid, 
    knn1 = predict(fit_knn1, newdata = grid, type="class"), 
    knn15 = predict(fit_knn15, newdata = grid, type="class"), 
    lda = predict(fit_lda, newdata = grid)$class, 
    qda = predict(fit_qda, newdata = grid)$class
  )

# Scatterplot of test set observations
# Overlay contour plot to draw decision boundaries
df_contours <- 
  boundary_df %>% 
  pivot_longer(cols = c(knn1, knn15, lda, qda),
               names_to = 'model',
               values_to = 'pred')

df_plot <- 
  bind_rows(
    sim_test_set %>% mutate('dataset' = 'Test'),
    sim_train_set %>% mutate('dataset' = 'Train')
  )

ggplot(df_plot) + 
  facet_wrap(~dataset) + 
  geom_point(aes(x = X1, y = X2, shape = Y)) +
  stat_contour(data = df_contours, aes(x = X1, y = X2, z = as.numeric(pred), color = model), 
               lwd = 1, breaks=c(1, 2, 3)) + 
  scale_shape_manual(values = c(16, 3)) +
  scale_color_discrete(name = "Model", 
                       labels = c("kNN (k=1)", "kNN (k=15)", "LDA", "QDA"))
```

**Solution:** 

The LDA decision boundary is a line and the QDA boundary is a quadratic curve, which is exactly what we would expect from these models. The kNN boundaries are allowed to be more flexible in shape. In particular, when k=1, kNN fits a very wiggly decision boundary with many islands (the nonsensical little circles), which indicates overfitting to training set. When k=15, the kNN boundary is smoother. 

Because we simulated the data, we know that the data was generated under the same assumptions as the LDA classifier. That is, the predictors X1 and X2 are multivariate normal with the same correlation structure for both classes. So it is no surprise that none of the more complicated models were able to outperform LDA in the test set, and that all of the boundaries look kind of linear. 
